{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Este jupyter es un copy paste de otro que github no me deja subir por ser demasiado grande, es lo mismo pero sin correr \n",
    "las celdas, alguna información relevante para entender el proceso la pondré según vea. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "PATH = ChromeDriverManager().install()     \n",
    "\n",
    "driver=webdriver.Chrome(PATH)      \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.common.keys import Keys \n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import regex as re\n",
    "\n",
    "from bs4 import BeautifulSoup as bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as req\n",
    "from pyshadow.main import Shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(URL)\n",
    "\n",
    "html_content = html.text\n",
    "\n",
    "\n",
    "soup=bs(html_content, 'html.parser')\n",
    "\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://archive.org/search?query=%28philosophy+%29&and%5B%5D=lending%3A%22is_readable%22&and%5B%5D=mediatype%3A%22texts%22&and%5B%5D=language%3A%22English%22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "BASE_URL = \"https://www.gutenberg.org/ebooks/search/?query=philosophy&submit_search=Go%21&start_index={}\"\n",
    "\n",
    "# Crear una lista con los índices de inicio para cada página\n",
    "start_indices = [i for i in range(26, 526, 25)]\n",
    "\n",
    "# Crear un conjunto de procesos en paralelo para extraer datos de todas las páginas\n",
    "\n",
    "results2 = Parallel(n_jobs=6)(delayed(extract_gut)(BASE_URL.format(start_index)) for start_index in start_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_gut(url):\n",
    "    res = req.get(url)\n",
    "    soup = bs(res.text, \"html.parser\")\n",
    "    autores = []\n",
    "    links_gut = []\n",
    "    for link in soup.find_all(\"a\", {\"class\": \"link\"}):\n",
    "        l = link.get(\"href\")\n",
    "        if len(l) < 15:\n",
    "            links_gut.append(\"https://www.gutenberg.org\" + l)\n",
    "\n",
    "    links_obras = []\n",
    "    for enlace in links_gut:\n",
    "        rec = req.get(enlace)\n",
    "        sopa = bs(rec.text, \"html.parser\")\n",
    "        for en in sopa.find_all(\"a\", {\"class\": \"link\"}):\n",
    "            l = en.get(\"href\")\n",
    "            if \".html.images\" in l:\n",
    "                try:\n",
    "                    for name in sopa.find_all(\"h1\"):\n",
    "                        autor = name.text\n",
    "                        autores.append(autor)\n",
    "                        links_obras.append(\"https://www.gutenberg.org\" + l)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    obras_autores = {}\n",
    "    for i, enla in enumerate(links_obras):\n",
    "        rec = req.get(enla)\n",
    "        sopa = bs(rec.text, \"html.parser\")\n",
    "        temp = [p.text for p in sopa.find_all(\"p\")]\n",
    "        obras_autores[autores[i]] = temp\n",
    "\n",
    "    return obras_autores\n",
    "\n",
    "BASE_URL = 'https://www.gutenberg.org/ebooks/search/?query=philosophy&submit_search=Go%21&start_index={}'\n",
    "\n",
    "# Crear una lista con los índices de inicio para cada página\n",
    "start_indices = [i for i in range(26, 526, 25)]\n",
    "\n",
    "# Crear un conjunto de procesos en paralelo para extraer datos de todas las páginas\n",
    "\n",
    "results2 = Parallel(n_jobs=6)(delayed(extract_gut)(BASE_URL.format(start_index)) for start_index in start_indices)\n",
    "df = pd.DataFrame(columns=[\"author\", \"title\", \"work\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"author\", \"title\", \"work\"])\n",
    "\n",
    "#en esta función limpiamos y separamos los libros enteros, quitamos las cosas raras que salgan de bajarlos y los ponemos\n",
    "#en un df\n",
    "\n",
    "def process_data(key, value):\n",
    "    split = key.split(\" by \")\n",
    "    if len(split) >= 2:\n",
    "        author = split[0]\n",
    "        title = split[1]\n",
    "        processed_data = []\n",
    "        for s in value:\n",
    "            s = s.replace('\\r', '').replace('\\n', '')  \n",
    "            processed_data.append({\"author\": author, \"title\": title, \"work\": s})\n",
    "        return processed_data\n",
    "    else:\n",
    "        print(\"Invalid format:\", key)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Parallel(n_jobs=6)(delayed(process_data)(k, v) for dic in results2 for k, v in dic.items())\n",
    "df = pd.concat([df, pd.DataFrame([item for sublist in results for item in sublist])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\Juan\\Desktop\\placeholder_name\\data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no todos los libros estaban en ingles\n",
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang\n",
    "    except:\n",
    "        return None\n",
    "merged_df['language'] = merged_df['work'].apply(detect_language)\n",
    "\n",
    "english_df = merged_df[merged_df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.shape, english_df.shape\n",
    "#aquí reducía de 383 a 298 filas, asumible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quitamos la columna de lenguaje\n",
    "#english_df = english_df.drop('language', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df.to_csv(r\"C:\\Users\\Juan\\Desktop\\placeholder_name\\dataen.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
